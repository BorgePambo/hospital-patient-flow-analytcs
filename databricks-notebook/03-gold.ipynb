{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c527e79f-8115-4308-bf71-75663bb07dbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit, col, expr, current_timestamp, to_timestamp, sha2, concat_ws, coalesce, monotonically_increasing_id\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ea124e5-b97d-4c95-b024-dc04cb42310d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ADLS configuration\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.hospitalstore11.dfs.core.windows.net\",\n",
    "    dbutils.secrets.get(scope=\"haspitalanalytcsvaultscopr\", key=\"storage-conn\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "782396c9-39c1-40ae-843d-f52c46c01d4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "\n",
    "silver_path = \"abfss://silver@hospitalstore11.dfs.core.windows.net/patient_flow\"\n",
    "gold_dim_patient = \"abfss://gold@hospitalstore11.dfs.core.windows.net/patient_flow/dim_patient\"\n",
    "gold_dim_department = \"abfss://gold@hospitalstore11.dfs.core.windows.net/patient_flow/dim_department\"\n",
    "gold_fact = \"abfss://gold@hospitalstore11.dfs.core.windows.net/patient_flow/fact_patient\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e21031e-d785-4c00-aa36-8d64922be148",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read silver data (assume append-only)\n",
    "silver_df = spark.read.format(\"delta\").load(silver_path)\n",
    "display(silver_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cd7c5e5-3fe7-4ca3-a0d9-9ac3386460f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define window for latest admission per patient\n",
    "w = Window.partitionBy(\"patient_id\").orderBy(F.col(\"admission_time\").desc())\n",
    "\n",
    "silver_df = (\n",
    "    silver_df\n",
    "    .withColumn(\"row_num\", F.row_number().over(w))  # Rank by latest admission_time\n",
    "    .filter(F.col(\"row_num\") == 1)                  # Keep only latest row\n",
    "    .drop(\"row_num\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e8e14b3-27dc-4938-90f3-cce6918c8d29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Patient Dimension Table Creation\n",
    "# Prepare incoming dimension records (deduplicated per patient, latest record)\n",
    "incoming_patient = (silver_df\n",
    "                    .select(\"patient_id\", \"gender\", \"age\")\n",
    "                    .withColumn(\"effective_from\", current_timestamp())\n",
    "                   )\n",
    "\n",
    "# Create target if not exists\n",
    "if not DeltaTable.isDeltaTable(spark, gold_dim_patient):\n",
    "    # initialize table with schema and empty data\n",
    "    incoming_patient.withColumn(\"surrogate_key\", F.monotonically_increasing_id()) \\\n",
    "                    .withColumn(\"effective_to\", lit(None).cast(\"timestamp\")) \\\n",
    "                    .withColumn(\"is_current\", lit(True)) \\\n",
    "                    .write.format(\"delta\").mode(\"overwrite\").save(gold_dim_patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0339d11-c73c-4375-ae9b-6cb1f2378519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load target as DeltaTable\n",
    "target_patient = DeltaTable.forPath(spark, gold_dim_patient)\n",
    "\n",
    "# Create an expression to detect attribute changes (hash or explicit comparisons)\n",
    "# We'll use a simple concat hash to detect changes\n",
    "incoming_patient = incoming_patient.withColumn(\n",
    "    \"_hash\",\n",
    "    F.sha2(F.concat_ws(\"||\", F.coalesce(col(\"gender\"), lit(\"NA\")), F.coalesce(col(\"age\").cast(\"string\"), lit(\"NA\"))), 256)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6a43dc2-d6d5-48f3-a8a8-975815715fef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Bring target current hash\n",
    "target_patient_df = spark.read.format(\"delta\").load(gold_dim_patient).withColumn(\n",
    "    \"_target_hash\",\n",
    "    F.sha2(F.concat_ws(\"||\", F.coalesce(col(\"gender\"), lit(\"NA\")), F.coalesce(col(\"age\").cast(\"string\"), lit(\"NA\"))), 256)\n",
    ").select(\"surrogate_key\", \"patient_id\", \"gender\", \"age\", \"is_current\", \"_target_hash\", \"effective_from\", \"effective_to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e608b841-441a-4390-8ea1-5cb32a8f982d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create temp views for merge\n",
    "incoming_patient.createOrReplaceTempView(\"incoming_patient_tmp\")\n",
    "target_patient_df.createOrReplaceTempView(\"target_patient_tmp\")\n",
    "\n",
    "# We'll implement in two steps using Delta MERGE (safe & explicit)\n",
    "\n",
    "# 1) Mark old current rows as not current where changed\n",
    "changes_df = spark.sql(\"\"\"\n",
    "SELECT t.surrogate_key, t.patient_id\n",
    "FROM target_patient_tmp t\n",
    "JOIN incoming_patient_tmp i\n",
    "  ON t.patient_id = i.patient_id\n",
    "WHERE t.is_current = true AND t._target_hash <> i._hash\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11dfc150-a8fb-4c14-8258-75451b7c8cd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "changed_keys = [row['surrogate_key'] for row in changes_df.collect()]\n",
    "\n",
    "if changed_keys:\n",
    "    # Update existing current records: set is_current=false and effective_to=current_timestamp()\n",
    "    target_patient.update(\n",
    "        condition = expr(\"is_current = true AND surrogate_key IN ({})\".format(\",\".join([str(k) for k in changed_keys]))),\n",
    "        set = {\n",
    "            \"is_current\": expr(\"false\"),\n",
    "            \"effective_to\": expr(\"current_timestamp()\")\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0e45bc8-e99c-4ae4-91d5-7ec684e86846",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2) Insert new rows for changed & new records\n",
    "# Build insert DF: join incoming with target to figure new inserts where either not exists or changed\n",
    "inserts_df = spark.sql(\"\"\"\n",
    "SELECT i.patient_id, i.gender, i.age, i.effective_from, i._hash\n",
    "FROM incoming_patient_tmp i\n",
    "LEFT JOIN target_patient_tmp t\n",
    "  ON i.patient_id = t.patient_id AND t.is_current = true\n",
    "WHERE t.patient_id IS NULL OR t._target_hash <> i._hash\n",
    "\"\"\").withColumn(\"surrogate_key\", F.monotonically_increasing_id()) \\\n",
    "  .withColumn(\"effective_to\", lit(None).cast(\"timestamp\")) \\\n",
    "  .withColumn(\"is_current\", lit(True)) \\\n",
    "  .select(\"surrogate_key\", \"patient_id\", \"gender\", \"age\", \"effective_from\", \"effective_to\", \"is_current\")\n",
    "\n",
    "# Append new rows\n",
    "if inserts_df.count() > 0:\n",
    "    inserts_df.write.format(\"delta\").mode(\"append\").save(gold_dim_patient)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e569ef2-808a-4611-b561-af856bdaee73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Department Dimension Table Creation\n",
    "\n",
    "# prepare incoming (latest per patient feed snapshot)\n",
    "incoming_dept = (silver_df\n",
    "                 .select(\"department\", \"hospital_id\")\n",
    "                )\n",
    "\n",
    "# add hash and dedupe incoming (one row per natural key)\n",
    "incoming_dept = incoming_dept.dropDuplicates([\"department\", \"hospital_id\"]) \\\n",
    "    .withColumn(\"surrogate_key\", monotonically_increasing_id())\n",
    "\n",
    "# initialize table if missing\n",
    "\n",
    "incoming_dept.select(\"surrogate_key\", \"department\", \"hospital_id\") \\\n",
    "    .write.format(\"delta\").mode(\"overwrite\").save(gold_dim_department)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fd01807-e81e-40aa-b8dc-dac02888a065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Fact table\n",
    "\n",
    "# Read current dims (filter is_current=true)\n",
    "dim_patient_df = (spark.read.format(\"delta\").load(gold_dim_patient)\n",
    "                  .filter(col(\"is_current\") == True)\n",
    "                  .select(col(\"surrogate_key\").alias(\"surrogate_key_patient\"), \"patient_id\", \"gender\", \"age\"))\n",
    "\n",
    "dim_dept_df = (spark.read.format(\"delta\").load(gold_dim_department)\n",
    "               .select(col(\"surrogate_key\").alias(\"surrogate_key_dept\"), \"department\", \"hospital_id\"))\n",
    "\n",
    "# Build base fact from silver events\n",
    "fact_base = (silver_df\n",
    "             .select(\"patient_id\", \"department\", \"hospital_id\", \"admission_time\", \"discharge_time\", \"bed_id\")\n",
    "             .withColumn(\"admission_date\", F.to_date(\"admission_time\"))\n",
    "            )\n",
    "\n",
    "# Join to get surrogate keys\n",
    "fact_enriched = (fact_base\n",
    "                 .join(dim_patient_df, on=\"patient_id\", how=\"left\")\n",
    "                 .join(dim_dept_df, on=[\"department\", \"hospital_id\"], how=\"left\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd41fc7b-9c44-48b4-b5f9-ca5f13eac9de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "fact_enriched = fact_enriched.withColumn(\"length_of_stay_hours\",\n",
    "                                         (F.unix_timestamp(col(\"discharge_time\")) - F.unix_timestamp(col(\"admission_time\"))) / 3600.0) \\\n",
    "                             .withColumn(\"is_currently_admitted\", F.when(col(\"discharge_time\") > current_timestamp(), lit(True)).otherwise(lit(False))) \\\n",
    "                             .withColumn(\"event_ingestion_time\", current_timestamp())\n",
    "\n",
    "# Let's make column names explicit instead:\n",
    "fact_final = fact_enriched.select(\n",
    "    F.monotonically_increasing_id().alias(\"fact_id\"),\n",
    "    col(\"surrogate_key_patient\").alias(\"patient_sk\"),\n",
    "    col(\"surrogate_key_dept\").alias(\"department_sk\"),\n",
    "    \"admission_time\",\n",
    "    \"discharge_time\",\n",
    "    \"admission_date\",\n",
    "    \"length_of_stay_hours\",\n",
    "    \"is_currently_admitted\",\n",
    "    \"bed_id\",\n",
    "    \"event_ingestion_time\"\n",
    ")\n",
    "\n",
    "# Persist fact table partitioned by admission_date (helps Synapse / queries)\n",
    "fact_final.write.format(\"delta\").mode(\"overwrite\").save(gold_fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12c160d5-b886-4cdc-9ea2-630915d235e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Quick sanity checks\n",
    "print(\"Patient dim count:\", spark.read.format(\"delta\").load(gold_dim_patient).count())\n",
    "print(\"Department dim count:\", spark.read.format(\"delta\").load(gold_dim_department).count())\n",
    "print(\"Fact rows:\", spark.read.format(\"delta\").load(gold_fact).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5f20315-b9cf-4d63-9677-c0fce4adbec2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(gold_fact)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8e67a03-d149-4f77-9b83-38c6741eb2db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dim_patient_df = spark.read.format(\"delta\").load(gold_dim_patient)\n",
    "display(dim_patient_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6687d134-16ee-415c-a1e0-bf89ac57e27c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dim_patient_df = spark.read.format(\"delta\").load(gold_dim_department)\n",
    "display(dim_patient_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33adf2a3-37b5-4173-8cd5-ed5d4fa5e167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0469b17-fc84-47c1-bc4a-de6192e41a82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8747500024800775,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03-gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
